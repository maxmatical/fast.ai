{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Numpy ML implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN6oHVwvtgkSqqcGh0FOwWz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxmatical/fast.ai/blob/master/Numpy_ML_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROV9WcRb67c-"
      },
      "source": [
        "# Neural Networks (and logistic regression)\n",
        "https://quantdare.com/create-your-own-deep-learning-framework-using-numpy/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgWdMk4N72Sc"
      },
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from typing import List, Optional, Tuple\n",
        "from enum import Enum, auto\n",
        "import numpy as np\n"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQklA5J6ncGH"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iafz777k8WKG"
      },
      "source": [
        "# A\n",
        "a =[0, 0, 1, 1, 0, 0,\n",
        "   0, 1, 0, 0, 1, 0,\n",
        "   1, 1, 1, 1, 1, 1,\n",
        "   1, 0, 0, 0, 0, 1,\n",
        "   1, 0, 0, 0, 0, 1]\n",
        "# B\n",
        "b =[0, 1, 1, 1, 1, 0,\n",
        "   0, 1, 0, 0, 1, 0,\n",
        "   0, 1, 1, 1, 1, 0,\n",
        "   0, 1, 0, 0, 1, 0,\n",
        "   0, 1, 1, 1, 1, 0]\n",
        "# C\n",
        "c =[0, 1, 1, 1, 1, 0,\n",
        "   0, 1, 0, 0, 0, 0,\n",
        "   0, 1, 0, 0, 0, 0,\n",
        "   0, 1, 0, 0, 0, 0,\n",
        "   0, 1, 1, 1, 1, 0]\n",
        "\n",
        "# Creating labels\n",
        "# y =[[1, 0, 0],\n",
        "#    [0, 1, 0],\n",
        "#    [0, 0, 1]]\n",
        "\n",
        "y = [1,2,3]"
      ],
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8le60oSe8ZSF",
        "outputId": "f894f974-3d17-43f8-f2ce-9af98e77fd8b"
      },
      "source": [
        "x =np.array([a, b, c])\n",
        "  \n",
        "  \n",
        "# Labels are also converted into NumPy array\n",
        "y = np.array(y)\n",
        "  \n",
        "  \n",
        "print(x, \"\\n\\n\", y)"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1]\n",
            " [0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0]\n",
            " [0 1 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0]] \n",
            "\n",
            " [1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbMsZARi8cLj",
        "outputId": "e99863f5-8958-4e22-e5f4-93115271ff32"
      },
      "source": [
        "x.shape, y.shape"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3, 30), (3,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EoFaSILwhnn"
      },
      "source": [
        "class Layer(ABC):\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, input):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def backward(self, input):\n",
        "        pass\n",
        "\n",
        "\n",
        "class LayerType(Enum):\n",
        "    # to keep track of layer types in NN\n",
        "    # layer type determines what is returned in the backward pass\n",
        "    linear = auto()\n",
        "    relu = auto()\n",
        "    sigmoid = auto()\n",
        "    identity = auto()\n",
        "\n",
        "\n",
        "# layers\n",
        "class Linear(Layer):\n",
        "    def __init__(self, in_dim: int, out_dim: int):\n",
        "        self.weights = np.random.normal(size=(out_dim, in_dim)) # rows x cols\n",
        "        # so when \n",
        "        self.biases = np.random.rand(out_dim, 1)\n",
        "        # keep track of layer type for computing gradients\n",
        "        self.type = \"linear\"\n",
        "        \n",
        "\n",
        "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
        "        # input into the layer is activations from previous layer\n",
        "        # keep track of previous activations for back prop\n",
        "        self.prev_activations = input \n",
        "        return np.matmul(self.weights, input) + self.biases\n",
        "\n",
        "\n",
        "    def backward(self, dA: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        by chain rule, dW is just equals to dA dot product with previous activations\n",
        "        \"\"\"\n",
        "        dW = np.dot(dA, self.prev_activations.T) # self.previous_activations.T for shape to match\n",
        "        dB = dA.mean(axis=1, keepdims=True) # average across columns (where dim = 1) to get dB\n",
        "        error = np.dot(self.weights.T, dA) # propagate error backwords by multiplying dA by weights\n",
        "        return error, dW, dB\n",
        "\n",
        "\n",
        "    def update(self, dW: np.ndarray, dB: np.ndarray, lr: float):\n",
        "        # update the weights and biases given gradients and lr\n",
        "        self.weights -= lr * dW\n",
        "        self.biases -= lr * dB\n",
        "\n",
        "\n"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3mbrKnL6ofe",
        "outputId": "e418a4cd-ac38-40d2-948b-510747e03203"
      },
      "source": [
        "l = Linear(30, 4)\n",
        "l.forward(x.T)"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.39297034,  0.83342806,  3.55624561],\n",
              "       [ 5.34235077,  6.24693926,  4.09729335],\n",
              "       [ 8.05260647,  5.36259176,  1.92813514],\n",
              "       [-1.52783894,  0.85110861,  0.13708511]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sH3VvyEiA-Ib",
        "outputId": "47262385-5661-4cfd-c748-e1b5e679bbfb"
      },
      "source": [
        "x.mean(axis=1, keepdims=True)"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.46666667],\n",
              "       [0.53333333],\n",
              "       [0.36666667]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcDj-9pNzJZB"
      },
      "source": [
        "# activations\n",
        "class ReLU(Layer):\n",
        "    def __init__(self):\n",
        "        # keep track of layer type for computing gradients\n",
        "        self.type = \"relu\"\n",
        "    \n",
        "\n",
        "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
        "        # keep track of activations for backprop\n",
        "        self.activations = np.maximum(0, input) \n",
        "        return self.activations\n",
        "\n",
        "\n",
        "    def backward(self, error: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        gradient of relu is 1 if self.activation > 0, 0 otherwise\n",
        "        \"\"\"\n",
        "        # gradients = np.copy(self.activations)\n",
        "        # gradients[self.activations > 0] = 1\n",
        "        # gradients[self.activations <= 0] = 0\n",
        "        # return error * gradients\n",
        "\n",
        "        \"\"\"\n",
        "        alternatively, use heaviside function\n",
        "        \"\"\"\n",
        "        return error * np.heaviside(self.activations, 0)\n",
        "        \n",
        "\n",
        "class Sigmoid(Layer):\n",
        "    def __init__(self):\n",
        "        # keep track of layer type for computing gradients\n",
        "        self.type = \"sigmoid\"\n",
        "\n",
        "\n",
        "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
        "        # keep track of activations for gradient calc in backward pass\n",
        "        self.activations = 1/(1 + np.exp(-input))\n",
        "        return self.activations\n",
        "\n",
        "\n",
        "    def backward(self, error: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        derivative of sigmoid function is sig*(1-sig)\n",
        "        \"\"\"\n",
        "        sig = self.activations\n",
        "        return error * sig * (1 - sig)\n",
        "\n",
        "    \n",
        "class Identity(Layer):\n",
        "    def __init__(self):\n",
        "        # keep track of layer type for computing gradients\n",
        "        self.type = \"identity\"\n",
        "\n",
        "    # identity function, just return itself\n",
        "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
        "        # keep track of activations for gradient calc in backward pass\n",
        "        self.activations = input\n",
        "        return self.activations\n",
        "\n",
        "    def backward(self, error: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        derivative of identity function is identity matrix\n",
        "        so error * I = error\n",
        "        \"\"\"\n",
        "        return error\n"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJRxZ5u_22Gp"
      },
      "source": [
        "# loss functions\n",
        "class LossType(Enum):\n",
        "    mse = auto()\n",
        "    bce = auto()\n",
        "\n",
        "\n",
        "class MSE(Layer):\n",
        "    def __init__(self, pred, y):\n",
        "        self.pred = pred\n",
        "        self.y = y\n",
        "\n",
        "\n",
        "    def forward(self) -> float:\n",
        "        return np.power(self.pred - self.y, 2).mean()\n",
        "\n",
        "\n",
        "    def backward(self) -> np.ndarray:\n",
        "        # need to keep same dimensions as pred and y\n",
        "        # to get loss wrt to each output\n",
        "        # since the 2 is a constant factor, we can leave out\n",
        "        # (equivalent to multiplying lr by 2)\n",
        "        return self.pred - self.y\n",
        "\n",
        "\n",
        "class BCE(Layer):\n",
        "    def __init__(self, pred, y):\n",
        "        self.pred = pred\n",
        "        self.y = y\n",
        "        self.n = len(self.y)\n",
        "\n",
        "\n",
        "    def forward(self) -> float:\n",
        "        loss = np.nansum(-self.y * np.log(self.pred) - (1 - self.y) * np.log(1 - self.pred)) / self.n\n",
        "        return np.squeeze(loss)\n",
        "\n",
        "\n",
        "    def backward(self) -> np.ndarray:\n",
        "        return (-(self.y / self.pred) + ((1 - self.y) / (1 - self.pred))) / self.n"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-tB_TU8B0UE",
        "outputId": "1572d125-bcd1-427d-b3b1-0634d6eb3e49"
      },
      "source": [
        "mse = MSE(y+5, y)\n",
        "mse.backward()"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 5, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtS0SSVcBfoC",
        "outputId": "de420db9-ba1e-4778-aaf3-bbd90935c5c8"
      },
      "source": [
        "bce = BCE(y+5, y)\n",
        "# bce.forward()\n",
        "bce.backward()"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.05555556, -0.08333333, -0.05555556])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAk9GXKGwCvA"
      },
      "source": [
        "class Model:\n",
        "    def __init__(self, in_dim: int, layers: Optional[List[int]], out_dim: int, loss_fn: str):\n",
        "        self.in_dim = in_dim # get input dimension for matmul, = X.shape[1]\n",
        "        self.layers=[]\n",
        "        self.loss_fn = loss_fn\n",
        "        assert self.loss_fn in [l.name for l in LossType], f\"error, loss {self.loss_fn} not recognized\"\n",
        "\n",
        "        if layers:\n",
        "            \"\"\"if layers not given, is equivalent to linear/logistic regression\n",
        "            \"\"\"\n",
        "            for l in layers:\n",
        "                # each layer is a feedforward + relu\n",
        "                self.layers.append(Linear(self.in_dim, l))\n",
        "                self.layers.append(ReLU())\n",
        "                self.in_dim = l\n",
        "        # add final classifier layer + activation\n",
        "        self.layers.append(Linear(self.in_dim, out_dim))\n",
        "        final_act_fn = Identity() if self.loss_fn == LossType.mse.name else Sigmoid()\n",
        "        self.layers.append(final_act_fn)\n",
        "\n",
        "\n",
        "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
        "        for l in self.layers:\n",
        "            input = l.forward(input)\n",
        "        return input\n",
        "        \n",
        "\n",
        "    def backward(self, pred: np.ndarray, y: np.ndarray, lr: float):\n",
        "        loss_func = MSE(pred, y) if self.loss_fn == LossType.mse.name else BCE(pred, y)\n",
        "        loss = loss_func.forward()\n",
        "        # compute gradients\n",
        "        gradient = loss_func.backward()\n",
        "\n",
        "        # backprop through the layers:\n",
        "        for i, l in reversed(list(enumerate(self.layers))):\n",
        "            if l.type != LayerType.linear.name:\n",
        "                # if not linear layer, just calculate the gradient and \n",
        "                # pass to the previous layer\n",
        "                gradient = self.layers[i].backward(gradient)\n",
        "            else:\n",
        "                gradient, dW, dB = self.layers[i].backward(gradient)\n",
        "                self.layers[i].update(dW, dB, lr)\n",
        "        return loss # keep track of loss\n",
        "\n",
        "\n",
        "    def fit(self, input: np.ndarray, y: np.ndarray, n_epochs: int, lr: float):\n",
        "        for _ in range(n_epochs):\n",
        "            out = self.forward(input)\n",
        "            loss = self.backward(out, y, lr)\n",
        "            print(f\"loss {self.loss_fn} = {loss}\")\n"
      ],
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Fi0fzZj6YLV",
        "outputId": "218cd8f3-bb58-4a70-aa48-5e72e094e1ec"
      },
      "source": [
        "nn = Model(in_dim=30, layers = [4,5,6], out_dim = 1, loss_fn=\"mse\")\n",
        "# use x.T to make shapes match\n",
        "out = nn.forward(x.T)\n",
        "out"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.81832437, 11.13986373,  6.87608558]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEftamXm_DWe",
        "outputId": "140c2dee-ca79-485d-ddc8-dbbbb058301d"
      },
      "source": [
        "nn.backward(out, y, lr=0.001)"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52.88598392077802"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gZoc3wHD2hR",
        "outputId": "ecd05489-cd6e-4b70-e3a0-ae773734c4d8"
      },
      "source": [
        "nn.fit(x.T, y, n_epochs=10, lr=0.001)"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss mse = 6.590479597024867\n",
            "loss mse = 3.5052972509792166\n",
            "loss mse = 2.2071097880932498\n",
            "loss mse = 1.505267191643277\n",
            "loss mse = 1.1115366041724635\n",
            "loss mse = 0.8224457792898495\n",
            "loss mse = 0.6212247369714167\n",
            "loss mse = 0.5148845118415777\n",
            "loss mse = 0.4116553871564776\n",
            "loss mse = 0.3422654683061006\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkAv0UFVEjNw",
        "outputId": "ed3fb5a0-661d-48b0-c7e8-2a03fbe0ffad"
      },
      "source": [
        "\"\"\"\n",
        "linear/logistic regression is equivalent to MLP\n",
        "just without any hidden layers\n",
        "\"\"\"\n",
        "log_reg = Model(in_dim=30, layers = None, out_dim = 1, loss_fn=\"bce\")\n",
        "log_reg.fit(x.T, y, n_epochs=10, lr=0.001)"
      ],
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss bce = 1.2452257546964574\n",
            "loss bce = 1.24474912884293\n",
            "loss bce = 1.24427293488365\n",
            "loss bce = 1.243797171968099\n",
            "loss bce = 1.2433218392470675\n",
            "loss bce = 1.2428469358726602\n",
            "loss bce = 1.2423724609982967\n",
            "loss bce = 1.2418984137787177\n",
            "loss bce = 1.2414247933699865\n",
            "loss bce = 1.2409515989294908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pHGuwJqFk0q"
      },
      "source": [
        "# Decision Tree/ Random Forests\n",
        "\n",
        "https://forums.fast.ai/t/unofficial-lesson-7-classnotes/7955"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NvDbRT0EpiC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}